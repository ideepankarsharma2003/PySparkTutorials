{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Repository: 'deb https://ppa.launchpadcontent.net/webupd8team/java/ubuntu/ jammy main'\n",
      "Description:\n",
      "The Oracle JDK License has changed for releases starting April 16, 2019.\n",
      "\n",
      "The new Oracle Technology Network License Agreement for Oracle Java SE is substantially different from prior Oracle JDK licenses. The new license permits certain uses, such as personal use and development use, at no cost -- but other uses authorized under prior Oracle JDK licenses may no longer be available. Please review the terms carefully before downloading and using this product. An FAQ is available here: https://www.oracle.com/technetwork/java/javase/overview/oracle-jdk-faqs.html\n",
      "\n",
      "Oracle Java downloads now require logging in to an Oracle account to download Java updates, like the latest Oracle Java 8u211 / Java SE 8u212. Because of this I cannot update the PPA with the latest Java (and the old links were broken by Oracle).\n",
      "\n",
      "For this reason, THIS PPA IS DISCONTINUED.\n",
      "\n",
      "UPDATE:\n",
      "\n",
      "For Oracle Java 17, see a different PPA -> https://www.linuxuprising.com/2021/09/how-to-install-oracle-java-17-lts-on.html\n",
      "\n",
      "Old description:\n",
      "\n",
      "Oracle Java (JDK) Installer (automatically downloads and installs Oracle JDK8). There are no actual Java files in this PPA.\n",
      "\n",
      "Important -> Why Oracle Java 7 And 6 Installers No Longer Work: http://www.webupd8.org/2017/06/why-oracle-java-7-and-6-installers-no.html\n",
      "\n",
      "Update: Oracle Java 9 has reached end of life: http://www.oracle.com/technetwork/java/javase/downloads/jdk9-downloads-3848520.html\n",
      "\n",
      "The PPA supports Ubuntu 18.10, 18.04, 16.04, 14.04 and 12.04.\n",
      "\n",
      "More info (and Ubuntu installation instructions):\n",
      "- http://www.webupd8.org/2012/09/install-oracle-java-8-in-ubuntu-via-ppa.html\n",
      "\n",
      "Debian installation instructions:\n",
      "- Oracle Java 8: http://www.webupd8.org/2014/03/how-to-install-oracle-java-8-in-debian.html\n",
      "More info: https://launchpad.net/~webupd8team/+archive/ubuntu/java\n",
      "Adding repository.\n",
      "Press [ENTER] to continue or Ctrl-c to cancel.^C\n",
      "Aborted.\n",
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Get:1 file:/var/cudnn-local-repo-ubuntu2204-8.9.4.25  InRelease [1572 B]\n",
      "Hit:2 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:1 file:/var/cudnn-local-repo-ubuntu2204-8.9.4.25  InRelease [1572 B]       \n",
      "Get:4 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:5 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:6 file:/var/cudnn-local-repo-ubuntu2204-8.9.5.29  InRelease [1572 B]       \n",
      "Get:6 file:/var/cudnn-local-repo-ubuntu2204-8.9.5.29  InRelease [1572 B]       \n",
      "Get:7 https://packages.microsoft.com/repos/edge stable InRelease [3569 B]      \n",
      "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:9 https://dl.cloudsmith.io/public/caddy/stable/deb/debian any-version InRelease [8263 B]\n",
      "Get:10 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1103 kB]\n",
      "Get:11 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-updates/main amd64 c-n-f Metadata [16.0 kB]\n",
      "Get:12 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [995 kB]\n",
      "Get:13 http://us-east-1.ec2.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 c-n-f Metadata [22.0 kB]\n",
      "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:15 https://packages.microsoft.com/repos/edge stable/main amd64 Packages [8505 B]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [896 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 c-n-f Metadata [11.4 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [793 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 c-n-f Metadata [16.8 kB]\n",
      "Fetched 4211 kB in 1s (4344 kB/s)               \n",
      "Reading package lists... Done\n",
      "W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_packages_microsoft_com_repos_edge-jammy.list:1 and /etc/apt/sources.list.d/microsoft-edge-dev.list:3\n",
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Package oracle-java8-installer is not available, but is referred to by another package.\n",
      "This may mean that the package is missing, has been obsoleted, or\n",
      "is only available from another source\n",
      "\n",
      "E: Package 'oracle-java8-installer' has no installation candidate\n"
     ]
    }
   ],
   "source": [
    "# !sudo add-apt-repository ppa:webupd8team/java\n",
    "# !sudo apt-get update\n",
    "# !sudo apt-get install oracle-java8-installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425364 sha256=aa05567f1d19a8247eb913dd6029432eed2d2d2c3abc4cc8f1ddc865fe9c61eb\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deepankar</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lina</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jordan</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessi</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bree</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  age\n",
       "0  Deepankar   20\n",
       "1       Lina   23\n",
       "2     Jordan   20\n",
       "3      Jessi   18\n",
       "4       Bree   16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas= pd.read_csv(\n",
    "    'demo.csv'\n",
    ")\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark session\n",
    "from pyspark.sql import  SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by bash)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 05:59:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark= SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-95-165.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9d72048c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark= spark.read.csv('demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|      _c0|_c1|\n",
      "+---------+---+\n",
      "|     name|age|\n",
      "|Deepankar| 20|\n",
      "|     Lina| 23|\n",
      "|   Jordan| 20|\n",
      "|    Jessi| 18|\n",
      "|     Bree| 16|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2= spark.read.option(\n",
    "                        'header',\n",
    "                        'true'\n",
    "                    ).csv(\n",
    "                          'demo.csv'\n",
    "                        )\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|Deepankar| 20|\n",
      "|     Lina| 23|\n",
      "|   Jordan| 20|\n",
      "|    Jessi| 18|\n",
      "|     Bree| 16|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Deepankar', age='20'),\n",
       " Row(name='Lina', age='23'),\n",
       " Row(name='Jordan', age='20'),\n",
       " Row(name='Jessi', age='18'),\n",
       " Row(name='Bree', age='16')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
